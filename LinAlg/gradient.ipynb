{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sympy as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar Valued Functions\n",
    "A multivariable function of dimension n that returns a scalar value:\n",
    "$$f: \\mathbb{R^n}\\rightarrow\\mathbb{R}$$\n",
    "\n",
    "### Example\n",
    "if we evaluate $f(x,y) = x+y$ at the point (2,1), we get the scalar value of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def f(x,y):\n",
    "    return x+y\n",
    "\n",
    "print(f(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "A vector composed of partial derivative of a scalar-valued function. The gradient measures the direction and the fastest rate of increase of a function at a given point.\n",
    "\n",
    "### Example\n",
    "Given $g(x,y) = 5x^2+3xy+3y^3$,  $x=2$, and $y=3$, \n",
    "\n",
    "$$\n",
    "\\nabla g = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial f}{\\partial x}\\\\\n",
    "        \\frac{\\partial f}{\\partial y}\\\\\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        10x+3y\\\\\n",
    "        3x+9y^2\\\\\n",
    "    \\end{bmatrix}= \n",
    "    \\begin{bmatrix}\n",
    "        29\\\\\n",
    "        87\\\\\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([29., 87.])\n"
     ]
    }
   ],
   "source": [
    "def g(input):\n",
    "    x, y = input\n",
    "    return 5 * x ** 2 + 3 * x * y + 3 * y ** 3\n",
    "\n",
    "input = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "output = g(input)\n",
    "output.backward()\n",
    "print(\"Gradient:\", input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian\n",
    "The derivative of a the gradient of scalar-valued function\n",
    "\n",
    "### Example\n",
    "Using our previous example\n",
    "\n",
    "$$\n",
    "H = \\nabla^2 g(x,y) = \n",
    "    \\begin{bmatrix}\n",
    "    g_{xx} & g_{xy}\\\\\n",
    "    g_{yx} & g_{yy}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 18y\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 54\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian: tensor([[10.,  3.],\n",
      "        [ 3., 54.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "output = g(input)\n",
    "grad = torch.autograd.grad(output, input, create_graph=True)[0]\n",
    "\n",
    "# Compute the Hessian\n",
    "hessian = torch.stack([torch.autograd.grad(grad[i], input, retain_graph=True)[0] for i in range(len(input))])\n",
    "\n",
    "print(\"Hessian:\", hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Determinant\n",
    "\n",
    "We can use the Hessian determinant $\\text{det}(H)$ to find the local maxima/minima of $g$. The rules are as follows:\n",
    "Given that $D(x,y) = \\text{det}(H(x,y)) = g_{xx}(x,y)g_{yy}(x,y)-(g_{xy}(x,y))^2$ and critical points $(a,b)$ such that $g_x(a,b) = g_y(a,b)=0$, then\n",
    "- If $D(a,b) > 0$ and $g_{xx} > 0$, then $(a,b)$ is a local minimum of $g$\n",
    "- If $D(a,b) > 0$ and $g_{xx} <> 0$, then $(a,b)$ is a local maximum of $g$\n",
    "- If $D(a,b) < 0$, then $(a,b)$ is a saddle point of $g$\n",
    "- If $D(a,b) = 0$, then the test is inconclusive.\n",
    "\n",
    "### Example\n",
    "We first find the critical points, by solving the system of equations in the gradient. \n",
    "$$10x+3y=0 \\Rightarrow y=-\\frac{10}{3}x$$\n",
    "$$3x+9y^2=0$$\n",
    "$$\\Rightarrow 3x+9(-\\frac{10}{3}x)^2=3x+9(\\frac{100}{9}x^2)=3x+100x^2=x(3+100x)=0$$\n",
    "$$\\Rightarrow x=0 \\text{ and } y=0 \\text{ or } x=-\\frac{3}{100} \\text{ and } y=(-\\frac{3}{100},\\frac{1}{10}) $$\n",
    "\n",
    "Since we already know the Hessian H, we can calculate its determinant using the following formula:\n",
    "$$\n",
    "\\text{det}\n",
    "    (\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "    \\end{bmatrix}) = ad-bc \\Rightarrow\n",
    "    (\\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 18y\n",
    "    \\end{bmatrix}) = 180y-9\n",
    "$$\n",
    "\n",
    "Next, we evaluate the determinant with the critical points.\n",
    " - At $(0,0)$:\n",
    "  $$\\text{det}(H) = 180(0)-9=-9$$\n",
    "  So, $(0,0)$ is a saddle point\n",
    " - At $(-\\frac{3}{100},\\frac{1}{10})$:\n",
    "  $$\\text{det}(H) = 180(\\frac{1}{10})-9=18-9=9$$\n",
    "  Since $f_xx = 10 > 0$, $(-\\frac{3}{100},\\frac{1}{10})$ is a local minimum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical points:\n",
      "(-3/100, 1/10)\n",
      "(0, 0)\n",
      "\n",
      "At point (-3/100, 1/10):\n",
      "Hessian determinant: 9.0\n",
      "This point is a local minimum\n",
      "\n",
      "At point (0, 0):\n",
      "Hessian determinant: -9.0\n",
      "This point is a saddle point\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return 5*x**2 + 3*x*y + 3*y**3\n",
    "\n",
    "def hessian(f, inputs):\n",
    "    return torch.autograd.functional.hessian(f, inputs)\n",
    "\n",
    "# Find critical points using SymPy\n",
    "x, y = sp.symbols('x y')\n",
    "fx = 10*x + 3*y\n",
    "fy = 3*x + 9*y**2\n",
    "solutions = sp.solve((fx, fy), (x, y))\n",
    "\n",
    "print(\"Critical points:\")\n",
    "for solution in solutions:\n",
    "    print(solution)\n",
    "\n",
    "# Evaluate Hessian at each critical point\n",
    "for solution in solutions:\n",
    "    x_crit = torch.tensor(float(solution[0]), requires_grad=True)\n",
    "    y_crit = torch.tensor(float(solution[1]), requires_grad=True)\n",
    "    \n",
    "    hessian_at_crit = hessian(f, (x_crit, y_crit))\n",
    "    hessian_tensor = torch.tensor([[hessian_at_crit[0][0], hessian_at_crit[0][1]],\n",
    "                                   [hessian_at_crit[1][0], hessian_at_crit[1][1]]])\n",
    "    hessian_det = torch.det(hessian_tensor)\n",
    "\n",
    "    print(f\"\\nAt point {solution}:\")\n",
    "    print(f\"Hessian determinant: {hessian_det.item()}\")\n",
    "\n",
    "    # Classify the critical point\n",
    "    if hessian_det > 0:\n",
    "        if hessian_tensor[0][0] > 0:\n",
    "            print(\"This point is a local minimum\")\n",
    "        else:\n",
    "            print(\"This point is a local maximum\")\n",
    "    elif hessian_det < 0:\n",
    "        print(\"This point is a saddle point\")\n",
    "    else:\n",
    "        print(\"The test is inconclusive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector-valued Functions\n",
    "A multivariable function of dimension n that returns a vector value of dimension n:\n",
    "$$h: \\mathbb{R^n}\\rightarrow\\mathbb{R^n}$$\n",
    "\n",
    "# The Jacobian\n",
    "The Jacobian matrixis a matrix that holds all first-order partial derivatives of a vector-valued function. The Jacobian form of a vector-valued function $h(f(x,y),g(x,y))$ is :\n",
    "$$\n",
    "\\textbf{J}h(f(x,y),g(x,y)) =\n",
    "    \\begin{bmatrix}\n",
    "    f_x & f_y\\\\\n",
    "    g_x & g_y\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "If \n",
    "$$\n",
    "f(x,y) =\n",
    "    \\begin{bmatrix}\n",
    "    \\sin(x)+y\\\\\n",
    "    x+\\cos(y)\n",
    "    \\end{bmatrix}\n",
    "\\Rightarrow f(\\pi,2\\pi) =\n",
    "    \\begin{bmatrix}\n",
    "    \\sin(\\pi)+2\\pi\\\\\n",
    "    \\pi+\\cos(2\\pi)\n",
    "    \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2\\pi\\\\\n",
    "\\pi+1\n",
    "\\end{bmatrix} \\approx\n",
    "\\begin{bmatrix}\n",
    "6.28\\\\\n",
    "4.14\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\textbf{J}(f) =\n",
    "    \\begin{bmatrix}\n",
    "    \\cos(x) & 1\\\\\n",
    "    1 & -\\sin(y)\n",
    "    \\end{bmatrix} \\Rightarrow  \\textbf{J}(f(\\pi,2\\pi)) =\n",
    "    \\begin{bmatrix}\n",
    "    \\cos(\\pi) & 1\\\\\n",
    "    1 & -\\sin(2\\pi)\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    -1 & 1\\\\\n",
    "    1 & 0\n",
    "    \\end{bmatrix} \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  tensor([6.2832, 4.1416], grad_fn=<StackBackward0>)\n",
      "Jacobian:  tensor([[-1.,  1.],\n",
      "        [ 1., -0.]])\n"
     ]
    }
   ],
   "source": [
    "# Define the input variables\n",
    "x = torch.tensor(torch.pi, requires_grad=True)\n",
    "y = torch.tensor(2*torch.pi, requires_grad=True)\n",
    "\n",
    "# Define the tensor function\n",
    "def f(x, y):\n",
    "    return torch.stack([torch.sin(x) + y, x + torch.cos(y)])\n",
    "\n",
    "# Calculate the output\n",
    "output = f(x, y)\n",
    "print(\"Output: \", output)\n",
    "\n",
    "# Calculate the Jacobian\n",
    "jacobian = torch.round(torch.stack(torch.autograd.functional.jacobian(f, (x, y))), decimals=4)\n",
    "\n",
    "print(\"Jacobian: \", jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Determinant\n",
    "\n",
    "The determinant of a Jacobian matrix can tell us the amplitude in which space contracts or expands during a transformation around a point. The Jacobian determinant at a point provides a measure of how much a function locally scales areas (in 2D), volumes (in 3D), or hyper-volumes (in higher dimensions) near that point.  The sign of the Jacobian determinant indicates whether the function preserves orientation (positive determinant) or reverses it (negative determinant). If the Jacobian determinant is non-zero at a point, the function is locally invertible at that point. This means there exists a local inverse function that maps back from the range to the domain around that point. \n",
    "\n",
    "To do this with a 3-dimensional Jacobian matrix, we can use the framework:\n",
    "\n",
    "$$\n",
    "\\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        a_1 & a_2 & a_3\\\\\n",
    "        b_1 & b_2 & b_3\\\\\n",
    "        c_1 & c_2 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") = a_1 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_2 & b_3\\\\\n",
    "        c_2 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") - a_2 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & b_3\\\\\n",
    "        c_1 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") - a_3 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & b_2\\\\\n",
    "        c_1 & c_2\\\\\n",
    "    \\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "\n",
    "### Example\n",
    "If \n",
    "$$\n",
    "f(x,y,z) =\n",
    "    \\begin{bmatrix}\n",
    "    x^2y\\\\\n",
    "    -y+z\\\\\n",
    "    x+z\n",
    "    \\end{bmatrix}\n",
    "\\Rightarrow f(3,1,0) =\n",
    "    \\begin{bmatrix}\n",
    "    9\\\\\n",
    "    -1\\\\\n",
    "    2\n",
    "    \\end{bmatrix} \n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\textbf{J}(f) =\n",
    "    \\begin{bmatrix}\n",
    "    2xy & x^2 & 0\\\\\n",
    "    0 & -1 & 1\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    \\end{bmatrix} \\Rightarrow  \\textbf{J}(f(3,1,0)) =\n",
    "    \\begin{bmatrix}\n",
    "    6 & 9 & 0\\\\\n",
    "    0 & -1 & 1\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Then the determinant of the Jacobian matrix can be found as follows:\n",
    "\n",
    "$$\n",
    "\\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        2xy & x^2 & 0\\\\\n",
    "        0 & -1 & 1\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") = 2xy \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        -1 & 1\\\\\n",
    "        0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") - x^2 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1\\\\\n",
    "        1 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") - 0 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        0 & -1\\\\\n",
    "        1 & 0\\\\\n",
    "    \\end{bmatrix}\n",
    ") \\\\\n",
    "= 2xy(−1(1)−(1(0)))−x^2(0(1)−1(1))+0(0(0)−(−1(1)))\\\\\n",
    "= 2xy(−1)−x^2(−1)+0\\\\\n",
    "= -2xy+x^2\n",
    "\\Rightarrow\n",
    "\\text{det}(\\textbf{J}(f(3,1,0))) = -2(3)(1)+(3)^2=3\n",
    "$$\n",
    "\n",
    "If we evaluated it at the point (1,0,2), we get the value of 1, which shows that the space does not change around this point. However if we evaluate the Jacobian determinant at (3,1,0), then we get a value of 3, indicating a tripling of local area, volume, or hyper-volume while maintaining orientation and ensuring local invertibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian determinant at point (3, 1, 0): 2.999999761581421\n"
     ]
    }
   ],
   "source": [
    "def f(input):\n",
    "    x, y, z = input\n",
    "    return torch.stack([x**2 * y, -y + z, x + z])\n",
    "\n",
    "def jacobian_det(func, input):\n",
    "    jac = torch.autograd.functional.jacobian(func, input)\n",
    "    return torch.det(jac)\n",
    "\n",
    "point = torch.tensor([3.0, 1.0, 0.0], requires_grad=True)\n",
    "det = jacobian_det(f, point)\n",
    "\n",
    "print(f\"Jacobian determinant at point (3, 1, 0): {det.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
