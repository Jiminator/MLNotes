{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar Valued Functions\n",
    "A multivariable function of dimension n that returns a scalar value:\n",
    "$$f: \\mathbb{R^n}\\rightarrow\\mathbb{R}$$\n",
    "\n",
    "### Example\n",
    "if we evaluate $f(x,y) = x+y$ at the point (2,1), we get the scalar value of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def f(x,y):\n",
    "    return x+y\n",
    "\n",
    "print(f(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "A vector composed of partial derivative of a scalar-valued function. The gradient measures the direction and the fastest rate of increase of a function at a given point.\n",
    "\n",
    "### Example\n",
    "Given $g(x,y) = 5x^2+3xy+3y^3$,  $x=2$, and $y=3$, \n",
    "\n",
    "$$\n",
    "\\nabla g = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial f}{\\partial x}\\\\\n",
    "        \\frac{\\partial f}{\\partial y}\\\\\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        10x+3y\\\\\n",
    "        3x+9y^2\\\\\n",
    "    \\end{bmatrix}= \n",
    "    \\begin{bmatrix}\n",
    "        29\\\\\n",
    "        87\\\\\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([29., 87.])\n"
     ]
    }
   ],
   "source": [
    "def g(input):\n",
    "    x, y = input\n",
    "    return 5 * x ** 2 + 3 * x * y + 3 * y ** 3\n",
    "\n",
    "input = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "output = g(input)\n",
    "output.backward()\n",
    "print(\"Gradient:\", input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian\n",
    "The derivative of a the gradient of scalar-valued function\n",
    "\n",
    "### Example\n",
    "Using our previous example\n",
    "\n",
    "$$\n",
    "H = \\nabla^2 g(x,y) = \n",
    "    \\begin{bmatrix}\n",
    "    g_{xx} & g_{xy}\\\\\n",
    "    g_{yx} & g_{yy}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 18y\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 54\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian: tensor([[10.,  3.],\n",
      "        [ 3., 54.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "output = g(input)\n",
    "grad = torch.autograd.grad(output, input, create_graph=True)[0]\n",
    "\n",
    "# Compute the Hessian\n",
    "hessian = torch.stack([torch.autograd.grad(grad[i], input, retain_graph=True)[0] for i in range(len(input))])\n",
    "\n",
    "print(\"Hessian:\", hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Determinant\n",
    "\n",
    "We can use the Hessian determinant $\\text{det}(H)$ to find the local maxima/minima of $g$. The rules are as follows:\n",
    "Given that $D(x,y) = \\text{det}(H(x,y)) = g_{xx}(x,y)g_{yy}(x,y)-(g_{xy}(x,y))^2$ and critical points $(a,b)$ such that $g_x(a,b) = g_y(a,b)=0$, then\n",
    "- If $D(a,b) > 0$ and $g_{xx} > 0$, then $(a,b)$ is a local minimum of $g$\n",
    "- If $D(a,b) > 0$ and $g_{xx} <> 0$, then $(a,b)$ is a local maximum of $g$\n",
    "- If $D(a,b) < 0$, then $(a,b)$ is a saddle point of $g$\n",
    "- If $D(a,b) = 0$, then the test is inconclusive.\n",
    "\n",
    "### Example\n",
    "We first find the critical points, by solving the system of equations in the gradient. \n",
    "$$10x+3y=0 \\Rightarrow y=-\\frac{10}{3}x$$\n",
    "$$3x+9y^2=0$$\n",
    "$$\\Rightarrow 3x+9(-\\frac{10}{3}x)^2=3x+9(\\frac{100}{9}x^2)=3x+100x^2=x(3+100x)=0$$\n",
    "$$\\Rightarrow x=0 \\text{ and } y=0 \\text{ or } x=-\\frac{3}{100} \\text{ and } y=(-\\frac{3}{100},\\frac{1}{10}) $$\n",
    "\n",
    "Since we already know the Hessian H, we can calculate its determinant using the following formula:\n",
    "$$\n",
    "\\text{det}\n",
    "    (\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "    \\end{bmatrix}) = ad-bc \\Rightarrow\n",
    "    (\\begin{bmatrix}\n",
    "    10 & 3\\\\\n",
    "    3 & 18y\n",
    "    \\end{bmatrix}) = 180y-9\n",
    "$$\n",
    "\n",
    "Next, we evaluate the determinant with the critical points.\n",
    " - At $(0,0)$:\n",
    "  $$\\text{det}(H) = 180(0)-9=-9$$\n",
    "  So, $(0,0)$ is a saddle point\n",
    " - At $(-\\frac{3}{100},\\frac{1}{10})$:\n",
    "  $$\\text{det}(H) = 180(\\frac{1}{10})-9=18-9=9$$\n",
    "  Since $f_xx = 10 > 0$, $(-\\frac{3}{100},\\frac{1}{10})$ is a local minimum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Determinant: Matrix([[10, 3], [3, 18*y]])\n",
      "\n",
      "Critical point: (-3/100, 1/10)\n",
      "D(H): 9\n",
      "f_xx: 10\n",
      "Nature: local minimum\n",
      "\n",
      "Critical point: (0, 0)\n",
      "D(H): -9\n",
      "f_xx: 10\n",
      "Nature: saddle point\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff, hessian, solve\n",
    "\n",
    "# Define the variables\n",
    "x, y = symbols('x y')\n",
    "\n",
    "# Define the function g(x, y)\n",
    "g = 5 * x**2 + 3 * x * y + 3 * y**3\n",
    "\n",
    "# Compute the first derivatives\n",
    "g_x = diff(g, x)\n",
    "g_y = diff(g, y)\n",
    "\n",
    "# Compute the critical points by solving the equations g_x = 0 and g_y = 0\n",
    "critical_points = solve([g_x, g_y], (x, y))\n",
    "\n",
    "# Compute the Hessian matrix\n",
    "H = hessian(g, (x, y))\n",
    "print(\"Hessian Determinant:\", H, end=\"\\n\\n\")\n",
    "\n",
    "# Evaluate the Hessian determinant at each critical point and determine the nature of each point\n",
    "for point in critical_points:\n",
    "    print(\"Critical point:\", point)\n",
    "    H_det = H.det().subs({x: point[0], y: point[1]})\n",
    "    print(\"D(H):\", H_det)\n",
    "    fxx = H[0, 0].subs({x: point[0], y: point[1]})\n",
    "    print(\"f_xx:\", fxx)\n",
    "    if H_det < 0:\n",
    "        nature = \"saddle point\"\n",
    "    elif H_det > 0:\n",
    "        if fxx > 0:\n",
    "            nature = \"local minimum\"\n",
    "        else:\n",
    "            nature = \"local maximum\"\n",
    "    else:\n",
    "        nature = \"inconclusive\"\n",
    "    print(\"Nature:\", nature, end=\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector-valued Functions\n",
    "A multivariable function of dimension n that returns a vector value of dimension n:\n",
    "$$h: \\mathbb{R^n}\\rightarrow\\mathbb{R^n}$$\n",
    "\n",
    "# The Jacobian\n",
    "The Jacobian matrixis a matrix that holds all first-order partial derivatives of a vector-valued function. The Jacobian form of a vector-valued function $h(f(x,y),g(x,y))$ is :\n",
    "$$\n",
    "\\textbf{J}h(f(x,y),g(x,y)) =\n",
    "    \\begin{bmatrix}\n",
    "    f_x & f_y\\\\\n",
    "    g_x & g_y\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "If \n",
    "$$\n",
    "f(x,y) =\n",
    "    \\begin{bmatrix}\n",
    "    \\sin(x)+y\\\\\n",
    "    x+\\cos(y)\n",
    "    \\end{bmatrix}\n",
    "\\Rightarrow f(\\pi,2\\pi) =\n",
    "    \\begin{bmatrix}\n",
    "    \\sin(\\pi)+2\\pi\\\\\n",
    "    \\pi+\\cos(2\\pi)\n",
    "    \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2\\pi\\\\\n",
    "\\pi+1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\textbf{J}(f) =\n",
    "    \\begin{bmatrix}\n",
    "    \\cos(x) & 1\\\\\n",
    "    1 & -\\sin(y)\n",
    "    \\end{bmatrix} \\Rightarrow  \\textbf{J}(f(\\pi,2\\pi)) =\n",
    "    \\begin{bmatrix}\n",
    "    \\cos(\\pi) & 1\\\\\n",
    "    1 & -\\sin(2\\pi)\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    -1 & 1\\\\\n",
    "    1 & 0\n",
    "    \\end{bmatrix} \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  tensor([6.2832, 4.1416], grad_fn=<StackBackward0>)\n",
      "Jacobian:  tensor([[-1.,  1.],\n",
      "        [ 1., -0.]])\n"
     ]
    }
   ],
   "source": [
    "# Define the input variables\n",
    "x = torch.tensor(torch.pi, requires_grad=True)\n",
    "y = torch.tensor(2*torch.pi, requires_grad=True)\n",
    "\n",
    "# Define the tensor function\n",
    "def f(x, y):\n",
    "    return torch.stack([torch.sin(x) + y, x + torch.cos(y)])\n",
    "\n",
    "# Calculate the output\n",
    "output = f(x, y)\n",
    "print(\"Output: \", output)\n",
    "\n",
    "# Calculate the Jacobian\n",
    "jacobian = torch.round(torch.stack(torch.autograd.functional.jacobian(f, (x, y))), decimals=4)\n",
    "\n",
    "print(\"Jacobian: \", jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Determinant\n",
    "\n",
    "The determinant of a Jacobian matrix can tell us the amplitude in which space contracts or expands during a transformation around a point. The Jacobian determinant at a point provides a measure of how much a function locally scales areas (in 2D), volumes (in 3D), or hyper-volumes (in higher dimensions) near that point.  The sign of the Jacobian determinant indicates whether the function preserves orientation (positive determinant) or reverses it (negative determinant). If the Jacobian determinant is non-zero at a point, the function is locally invertible at that point. This means there exists a local inverse function that maps back from the range to the domain around that point. \n",
    "\n",
    "To do this with a 3-dimensional Jacobian matrix, we can use the framework:\n",
    "\n",
    "$$\n",
    "\\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        a_1 & a_2 & a_3\\\\\n",
    "        b_1 & b_2 & b_3\\\\\n",
    "        c_1 & c_2 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") = a_1 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_2 & b_3\\\\\n",
    "        c_2 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") - a_2 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & b_3\\\\\n",
    "        c_1 & c_3\\\\\n",
    "    \\end{bmatrix}\n",
    ") - a_3 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & b_2\\\\\n",
    "        c_1 & c_2\\\\\n",
    "    \\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "\n",
    "### Example\n",
    "If \n",
    "$$\n",
    "f(x,y,z) =\n",
    "    \\begin{bmatrix}\n",
    "    x^2y\\\\\n",
    "    -y+z\\\\\n",
    "    x+z\n",
    "    \\end{bmatrix}\n",
    "\\Rightarrow f(3,1,0) =\n",
    "    \\begin{bmatrix}\n",
    "    9\\\\\n",
    "    -1\\\\\n",
    "    2\n",
    "    \\end{bmatrix} \n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\textbf{J}(f) =\n",
    "    \\begin{bmatrix}\n",
    "    2xy & x^2 & 0\\\\\n",
    "    0 & -1 & 1\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    \\end{bmatrix} \\Rightarrow  \\textbf{J}(f(3,1,0)) =\n",
    "    \\begin{bmatrix}\n",
    "    6 & 9 & 0\\\\\n",
    "    0 & -1 & 1\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Then the determinant of the Jacobian matrix can be found as follows:\n",
    "\n",
    "$$\n",
    "\\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        2xy & x^2 & 0\\\\\n",
    "        0 & -1 & 1\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") = 2xy \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        -1 & 1\\\\\n",
    "        0 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") - x^2 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1\\\\\n",
    "        1 & 1\\\\\n",
    "    \\end{bmatrix}\n",
    ") - 0 \\text{det}(\n",
    "    \\begin{bmatrix}\n",
    "        0 & -1\\\\\n",
    "        1 & 0\\\\\n",
    "    \\end{bmatrix}\n",
    ") \\\\\n",
    "= 2xy(−1(1)−(1(0)))−x^2(0(1)−1(1))+0(0(0)−(−1(1)))\\\\\n",
    "= 2xy(−1)−x^2(−1)+0\\\\\n",
    "= -2xy+x^2\n",
    "$$\n",
    "\n",
    "If we evaluated it at the point (1,0,2), we get the value of 1, which shows that the space does not change around this point. HOwever if we evaluate the Jacobian determinant at (3,1,0), then we get a value of 3, indicating a tripling of local area, volume, or hyper-volume while maintaining orientation and ensuring local invertibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian:  tensor([[ 6.,  9.,  0.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 1.,  0.,  1.]])\n",
      "Jacobian Determinant:  2.999999761581421\n"
     ]
    }
   ],
   "source": [
    "# Correctly defining the function with operations suitable for automatic differentiation\n",
    "def f_final(x, y, z):\n",
    "    return torch.stack([x**2 * y, -y + z, x + z])\n",
    "\n",
    "# Define the variables x, y, z with gradient tracking\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Evaluate the function f at the point (3, 1, 0)\n",
    "output = f_final(x, y, z)\n",
    "\n",
    "# Create a Jacobian matrix\n",
    "jacobian = torch.zeros((3, 3), dtype=torch.float)\n",
    "\n",
    "# Compute the Jacobian matrix by backpropagating from each output component\n",
    "for i in range(3):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    if y.grad is not None:\n",
    "        y.grad.zero_()\n",
    "    if z.grad is not None:\n",
    "        z.grad.zero_()\n",
    "\n",
    "    output[i].backward(retain_graph=True)\n",
    "    jacobian[i] = torch.tensor([x.grad, y.grad, z.grad])\n",
    "\n",
    "# Compute the determinant of the Jacobian matrix\n",
    "jacobian_det = torch.det(jacobian)\n",
    "\n",
    "\n",
    "print(\"Jacobian: \", jacobian)\n",
    "print(\"Jacobian Determinant: \", jacobian_det.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
