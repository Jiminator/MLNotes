{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "SVD factorizes of a real or complex matrix into a rotation, followed by a rescaling followed by another rotation. It is colloquially known as the swiss-army knife of linear algebra due to its versatility and many places in application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prereqs\n",
    "### Eigenvector\n",
    "An eigenvector of a square matrix $A$ is defined as a vector satisfying the equation\n",
    "$$A\\vec{x}=\\lambda\\vec{x}$$\n",
    "and $\\lambda$ is the corresponding eigenvalue. In other words, an eigenvector of A$$ is any vector that, when\n",
    "multiplied by $A$, comes back as itself scaled by $\\lambda$.\n",
    "\n",
    "### Symmetric Positive Definite\n",
    "A matrix is PSD if all of its eigenvalues $\\geq$ 0, or equivalently a matrix $A$ for which $\\vec{x}^TA\\vec{x} \\geq 0$ for any vector \\$vec{x}$. To generate a $n \\times n$ positive semi-definite matrix, we can take any matrix $X$ that has n columns and let $A = X^TX$\n",
    "\n",
    "\n",
    "### Spectral Theorem\n",
    "If $A$ is symmetric positive definite matrix, A can be decomposed into $A = S\\lambda S^T$, where is S is a orthogonal matrix that contains the eigenvectors, $\\lambda$ is a diagonal matrix filled with positive eigenvalues.\n",
    "\n",
    "\n",
    "### Orthogonal Matrix \n",
    "An orthogonal matrix is a real square matrix whose columns and rows are orthonormal vectors, which means that the vectors are unit vectors and are orthogonal to each other. In other words, if $A$ is  orthogonal matrix, $A^TA = AA^T = I$. For example, consider \n",
    "$A = \n",
    "\\begin{bmatrix}\n",
    "\\cos(x) & \\sin(x)\\\\\n",
    "-\\sin(x) & \\cos(x)\n",
    "\\end{bmatrix}\n",
    "$, \n",
    "$$\n",
    "AA^T = \n",
    "\\begin{bmatrix}\n",
    "\\cos(x) & \\sin(x)\\\\\n",
    "-\\sin(x) & \\cos(x)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\cos(x) & -\\sin(x)\\\\\n",
    "\\sin(x) & \\cos(x)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\cos^2(x) + \\sin^2(x) & -\\cos(x)\\sin(x)+\\sin(x)\\cos(x)\\\\\n",
    "-\\sin(x)\\cos(x)+\\cos(x)\\sin(x) & \\sin^2(x)+\\cos^2(x)\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Some properties of orthogonal matrix A include:\n",
    "-  Transpose and Inverse are equal. i.e. $A^{-1}=A^T$\n",
    "-  The product of A and its transpose is an identity matrix. i.e $A^TA = AA^T = I$\n",
    "-  Determinant is $det(A) = \\pm 1$ Thus, an orthogonal matrix is always non-singular as its determinant is not 0.\n",
    "-  A diagonal matrix with elements to be 1 or -1 is always orthogonal\n",
    "-  $A^T$ is also orthogonal. Since $A^-1=A^T$, $A^{-1}$ is also orthogonal\n",
    "-  The eigenvalues of $A$ are $\\pm 1$ and the eigenvectors are orthogonal\n",
    "-  An identity matrix (I) is orthogonal.\n",
    "-  Orthogonal matrices applied as a transformation preserves the length of vectors, and the angles between vectors. In other words, $||Av|| = ||v||$, and $(Au)\\cdot(Av) = u \\cdot v$\n",
    "-  In two dimensions, a rotation matrix $R(\\theta)$ that rotates vectors counter clockwise by an angle $\\theta$ is given by \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta)\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- In 3D, orthogonal matrices (with determinant 1) represent rotations around an axis. For example, the rotation matrix around the $z$-axis by an angle $\\theta$ is given by \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- They can represent pure rotations (when the determinant is 1) or reflections combined with rotations (when the determinant is -1)\n",
    "- Orthogonal matrices preserve orthonormality: If the original vectors are orthonormal, their images under an orthogonal transformation remain orthonormal\n",
    "- Because orthogonal matrices preserve lengths and angles, they perform transformations that resemble rotating vectors around the origin without changing their shape or size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal Matrix\n",
    "A diagonal matrix is a matrix in which the entries outside the main diagonal are all zero. Diagonal matrices can be thought of as stretches because they scale each component of a vector independently along the corresponding coordinate axis.\n",
    "\n",
    "\n",
    "Some properties of diagonal matrix include:\n",
    "- Diagonal matrices are typically square matrices, but not always. So long as all the non-zero elements have the same row & column index, the matrix is diagonal. A square diagonal matrix can have extra zero rows and/or columns added and still stay diagonal.\n",
    "- The sum of two diagonal matrices is a diagonal matrix\n",
    "- The product of two diagonal matrices is a diagonal matrix where the elements of its principal diagonal are the products of corresponding elements\n",
    "- Diagonal matrices are commutative under both addition and multiplication.\n",
    "- The rank is the number of non-zeroes on the diagonal.\n",
    "- The eigenvalues are just the diagonal numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "Eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. It can only be done of a square diagonalizable matrix. \n",
    "This can be determined in a few ways after computing the eigenvalues and eigenvectors\n",
    " - Check that for each eigenvalue, its number of times it appears as a root of the characteristic polynomial is the same as the number of linearly independent eigenvectors associated with that eigenvalue A.\n",
    " - Check if you have $n$ linearly independent eigenvectors \n",
    "   - If you have n-distinct eigenvalues, this holds true\n",
    "- Check if the Jordan of the matrix is diagonal\n",
    "- Check if the matrix is symmetric\n",
    "\n",
    "If A is symmetric, then there exists $S$ and $\\Lambda$ such that $A=S\\Lambda S^T$ because for symmetric A the eigenvectors in S are orthonormal, so S is Orthogonal.\n",
    "\n",
    "The intuition is as follows. Supposed we have a $n$ linearly independent eigenvectors $x_i$ of A\n",
    "Let's put them in columns of a matrix S - eigenvectors\n",
    "$$S = \n",
    "\\begin{bmatrix}\n",
    "| & & |\\\\\n",
    "x_1 & \\cdots & x_n\\\\\n",
    "| & & |\\\\\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "Now what if we multiply $AS$?\n",
    "Since all these $x_i$ are eigenvectors, $Ax_i=\\lambda_ix_i$\n",
    "Thus $AS = A\n",
    "\\begin{bmatrix}\n",
    "| & | & & |\\\\\n",
    "x_1 & x_2 & & \\cdots & x_n\\\\\n",
    "| & | & & |\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "| & | & & |\\\\\n",
    "\\lambda_1 x_1 & \\lambda_2 x_2 & & \\cdots & \\lambda_m x_n\\\\\n",
    "| & | & & |\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "| & | & & |\\\\\n",
    "x_1 & x_2 & & \\cdots & x_n\\\\\n",
    "| & | & & |\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Let's call the diagonal matrix $\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "so $AS = S\\Lambda \\Rightarrow A = S\\Lambda A^{-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations of SVD\n",
    "Every $m \\times n$ matrix can be factored into $A_{m \\times n}=U_{m \\times m}\\Sigma_{m \\times n} V^T_{n \\times n}$. The values in $\\Sigma$ are called the singular values. The columns in $U$ are known as left singular vectors, and the rows in $V^T$ are known as the left singular vectors. \n",
    "\n",
    "SVD just states that all matrices are a sequential application of a three linear transformations. $V^T$ is a orthonormal matrix that applies a rotation such that the right singular vectors return to the standard basis.  $\\Sigma$ is a rectangularly diagonal matrix composed of a dimension eraser that reduces the dimensionality from $R^n$ to $R^m$. It also performs a stretch on each axis by the corresponding singular values, and the final step, orthogonal matrix $U$ rotates the standard basis to the left singular vectors.\n",
    "\n",
    "Another interpretation of SVD is that a high rank matrix $A$ is a summation of k rank-1 matrices, where $A = \\sum_{i=1}^{k}\\sigma_{i}u_iv_i^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the SVD tries to solve\n",
    "\n",
    "Problem: Given matrix $A$, we want to find a set of orthogonal set of a vectors that when transformed by our matrix will remain orthogonal. We can write down the problem in 2-d space with the equations below:\n",
    "$$Av_1=y_1$$\n",
    "$$Av_2=y_2$$\n",
    "We can split up the y values into a direction and magnitude so that $u_1$ and $u_2$ are unit vectors.\n",
    "$$Av_1=\\sigma_1u_1$$\n",
    "$$Av_2=\\sigma_2u_2$$\n",
    "We can rewrite this equation in matrix form to make it more general.\n",
    "$$A \n",
    "\\begin{bmatrix}\n",
    "v_1 & v_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1u_1 & \\sigma_2u_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "u_1 & u_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & 0\\\\\n",
    "0 & \\sigma_2\n",
    "\\end{bmatrix} = U\\Sigma \\Rightarrow A = U\\Sigma V^{-1} = U\\Sigma V^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the SVD\n",
    "To find U, we will multiply both sides by $A^T$. Since U is a orthogonal matrix, $U^T = U^{-1}$ \n",
    "$$A^TA=(U\\Sigma V^T)^T(U\\Sigma V^T)=V\\Sigma U^TU\\Sigma V^T=V\\Sigma^2V^T$$\n",
    "So know we know V is given by the eigendecomposition of $A^TA$, and out condition that V be orthogonal is satisfied because $A^TA$ is symmetric, and the eigenvectors of a symmetric matrix are orthogonal\n",
    "\n",
    "Similarly,\n",
    "$$AA^T=(U\\Sigma V^T)(U\\Sigma V^T)^T=U\\Sigma V^TV\\Sigma U^T=U\\Sigma^2U^T$$\n",
    "So know we know U is given by the eigendecomposition of $AA^T$, and out condition that U be orthogonal is satisfied because $AA^T$ is symmetric, and the eigenvectors of a symmetric matrix are orthogonal\n",
    "\n",
    "### Example\n",
    "Find the singular value decomposition of the matrix $C = \\begin{bmatrix} 5 & 5 \\\\ -1 & 7\\end{bmatrix}$\n",
    "We want $C=U\\Sigma V^T$, and $CV=U\\Sigma$\n",
    "\n",
    "**Finding V**\n",
    "$$C^TC=V\\Sigma^T \\Sigma V^T = \\begin{bmatrix} 5 & -1 \\\\ 5 & 7\\end{bmatrix} \\begin{bmatrix} 5 & 5 \\\\ -1 & 7\\end{bmatrix} = \\begin{bmatrix} 26 & 18 \\\\ 18 & 74\\end{bmatrix}$$\n",
    "\n",
    "To find the eigenvectors and eigenvalues of $A^TA$, we do the following\n",
    "$$\n",
    "\\text{det}(C^TC-\\lambda I) = \\text{det}\\left(\\begin{bmatrix} 26-\\lambda & 18 \\\\ 18 & 74-\\lambda\\end{bmatrix}\\right) = \\lambda^2-100\\lambda+1600 = (\\lambda-20)(\\lambda-80)\n",
    "$$ \n",
    "So we know the eigenvalues are 20 and 80.\n",
    "To find the eigenvector for $\\lambda = 20$\n",
    "$$C^TC-20I= \\begin{bmatrix} 6 & 18 \\\\ 18 & 54\\end{bmatrix}$$\n",
    "Since the second column is three times the first column, We can see that the null space can be $\\begin{bmatrix} -3 & 1\\end{bmatrix}$, but we want a unit vector so find the null space to be $\\begin{bmatrix} \\frac{-3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}}\\end{bmatrix}$\n",
    "\n",
    "To find the eigenvector for $\\lambda = 80$\n",
    "$$C^TC-80I= \\begin{bmatrix} -54 & 18 \\\\ 18 & -6\\end{bmatrix}$$\n",
    "The null space is then  $\\begin{bmatrix} \\frac{1}{\\sqrt{10}} & \\frac{3}{\\sqrt{10}}\\end{bmatrix}$.\n",
    "\n",
    "Thus, $V^T = \\begin{bmatrix} \\frac{1}{\\sqrt{10}} & \\frac{3}{\\sqrt{10}}\\\\ \\frac{-3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}}\\end{bmatrix}$ and $\\Sigma = \\begin{bmatrix} 4\\sqrt{5} & 0 \\\\ 0 & 2\\sqrt{5}\\end{bmatrix}$\n",
    "\n",
    "**Finding U**\n",
    "$$CV=U\\Sigma= \\begin{bmatrix} 5 & 5 \\\\ -1 & 7\\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{10}} & \\frac{-3}{\\sqrt{10}}\\\\ \\frac{3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}}\\end{bmatrix} = \\begin{bmatrix} 2\\sqrt{10} & -\\sqrt{10}\\\\ 2\\sqrt{10} & \\sqrt{10}\\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\begin{bmatrix} 4\\sqrt{5} & 0 \\\\ 0 & 2\\sqrt{5}\\end{bmatrix}$$\n",
    "\n",
    "Thus, \n",
    "$\n",
    "U = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$$\n",
    "C = \n",
    "\\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix}\n",
    "\\begin{bmatrix} 4\\sqrt{5} & 0 \\\\ 0 & 2\\sqrt{5}\\end{bmatrix}\n",
    " \\begin{bmatrix} \\frac{1}{\\sqrt{10}} & \\frac{3}{\\sqrt{10}}\\\\ \\frac{-3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}}\\end{bmatrix} = U\\Sigma V^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix C:\n",
      " [[ 5  5]\n",
      " [-1  7]]\n",
      "Reconstructed C:\n",
      " [[ 5.  5.]\n",
      " [-1.  7.]]\n",
      "U:\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "Sigma:\n",
      " [[8.94427191 0.        ]\n",
      " [0.         4.47213595]]\n",
      "VT:\n",
      " [[ 0.31622777  0.9486833 ]\n",
      " [-0.9486833   0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrix C\n",
    "C = np.array([[5, 5],\n",
    "              [-1, 7]])\n",
    "\n",
    "# Step 1: Compute C^T C\n",
    "CTC = C.T @ C\n",
    "\n",
    "# Step 2: Compute eigenvalues and eigenvectors for V and Sigma\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(CTC)\n",
    "# Sorting eigenvalues and corresponding eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "V = eigenvectors[:, sorted_indices]\n",
    "Sigma = np.sqrt(np.diag(eigenvalues[sorted_indices]))\n",
    "\n",
    "# Step 3: Compute U\n",
    "U = C @ V @ np.linalg.inv(Sigma)\n",
    "\n",
    "# Normalizing U's columns to ensure they are unit vectors\n",
    "U = U / np.linalg.norm(U, axis=0)\n",
    "\n",
    "# Verify the decomposition C = U Sigma V^T\n",
    "C_reconstructed = U @ Sigma @ V.T\n",
    "\n",
    "print(\"Matrix C:\\n\", C)\n",
    "print(\"Reconstructed C:\\n\", C_reconstructed)\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Sigma:\\n\", Sigma)\n",
    "print(\"VT:\\n\", V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix C:\n",
      "tensor([[ 5.,  5.],\n",
      "        [-1.,  7.]])\n",
      "\n",
      "Matrix U:\n",
      "tensor([[ 0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]])\n",
      "\n",
      "Singular Values (S):\n",
      "tensor([8.9443, 4.4721])\n",
      "\n",
      "Matrix VT:\n",
      "tensor([[ 0.3162,  0.9487],\n",
      "        [ 0.9487, -0.3162]])\n",
      "\n",
      "Reconstructed C (UΣV^T):\n",
      "tensor([[ 5.0000,  5.0000],\n",
      "        [-1.0000,  7.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Define the matrix C\n",
    "C = torch.tensor([[5.0, 5.0], [-1.0, 7.0]])\n",
    "\n",
    "# Step 2: Compute the SVD using PyTorch's built-in function\n",
    "U, S, V = torch.svd(C)\n",
    "\n",
    "# Display the results\n",
    "print(\"Matrix C:\")\n",
    "print(C)\n",
    "print(\"\\nMatrix U:\")\n",
    "print(U)\n",
    "print(\"\\nSingular Values (S):\")\n",
    "print(S)\n",
    "print(\"\\nMatrix VT:\")\n",
    "print(V.T)\n",
    "\n",
    "# Reconstruct C using U, S, V^T\n",
    "S_matrix = torch.diag(S)\n",
    "C_reconstructed = torch.mm(U, torch.mm(S_matrix, V.t()))\n",
    "print(\"\\nReconstructed C (UΣV^T):\")\n",
    "print(C_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Find the direction of maximum stretching\n",
    "Problem: In what direction does matrix $A$ stretch a unit vector the most? or $$\\underset{x:||x||=1}{\\arg\\max} ||Ax|| \\equiv \\underset{x:||x||=1}{\\arg\\max} ||Ax||^2 = \\underset{x:||x||=1}{\\arg\\max} (Ax)^T(Ax) = \\underset{x:||x||=1}{\\arg\\max}  x^TA^TAx$$\n",
    "\n",
    "Now lets write out the eigenvalue expression of A^TA\n",
    "$$\n",
    "(A^TA)x=\\lambda x\\\\\n",
    "x^T(A^TA)x=\\lambda x^Tx=\\lambda\\\\\n",
    "$$\n",
    "This shows that to maximize this quantity, choose x to be the eigenvector with the maximum eigenvalue. The eigenvector of $A^TA$ with the largest eigenvalue is the first column of U, thus\n",
    "$$\\underset{x:||x||=1}{\\arg\\max}  x^TA^TAx = u_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application from Strang's Book\n",
    "- If we had a spreadsheet that contains the grades for all courses, there would be a column for each student and a row for each course: The entry $a_{ij}$ would be the grade.Then $\\sigma_1u_1v_1^T$ could have $u_1 = \\text{combination course}$ and $v_1 = \\text{combination student}$. And $\\sigma_1$ would be the grade for those combinations: the highest grade.\n",
    "- The matrix $A$ could count the frequency of key words in a journal : A different article for each column of $A$ and a different word for each row. The whole journal is indexed by the matrix A and the most important information is in $\\sigma_1u_1v_1^T$  Then $\\sigma_1$ is the largest frequency for a hyperword (the word combination $u_1$ )in the hyperarticle $v_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "Given rank $r$ of matrix $A$\n",
    "- the vectors $v_1, ..., v_r$ is the orthonormal basis for the row space.\n",
    "- the vectors $u_1, ..., u_r$ is the orthonormal basis for the column space.\n",
    "- the vectors $v_{r+1}, ..., v_n$ is the orthonormal basis for the null space.\n",
    "- the vectors $u_{r+1}, ..., v_m$ is the orthonormal basis for the null space of $A^T$ (left nullspace).\n",
    "- $Av_i=\\sigma _iu_i$\n",
    "- The singular values in $\\Sigma$ are ordered in descending order in magnitude ($\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r > 0$)\n",
    "- The number of nonzero $\\sigma$ values in $\\Sigma$ give us the rank\n",
    "- The numbers $\\sigma_1^2$, $\\sigma_2^2$, ..., $\\sigma_r^2$ are the nonzero eigenvalues of $AA^T$ and $A^TA$\n",
    "- $A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + ... +\\sigma_ru_rv_r^T$ and $\\sigma_1$ is the maximum of the ratio $\\frac{||Ax||}{||x||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.youtube.com/watch?v=mBcLRGuAFUk&t=44s\n",
    "- https://www.cuemath.com/algebra/orthogonal-matrix/\n",
    "- https://www.youtube.com/watch?v=vSczTbgc8Rc&t=400s\n",
    "- https://www.youtube.com/watch?v=CpD9XlTu3ys\n",
    "- https://www.youtube.com/watch?v=cOUTpqlX-Xs\n",
    "- https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03a_SVDandLinSys.pdf\n",
    "- https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf#page=1.49"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
